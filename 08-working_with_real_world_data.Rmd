# Working with Real-World Data 
## Handling Large Datasets 
### Challenges of Large Data sets 
When working with data in R, one can encounter large data sets that are challenging to work on. These are some of the challenges; 

- Data I/O takes a long time such that large files takes a long time to transfer data to or from a computers system which will slow down crucial processes like network operations, communication between the devices such as keyboard and microphone, and sharing data
- R has a file size limit of approximately 2 to 4gb, therefore it is a challenge to work on data sets above the limit like 5gb.
- There is more difficulty in indexing data sets which extremely large number of rows and columns. 
- The processing speed of several algorithms and pre-trained model will reduce.
- Large data sets pose a threat on the memory management. R stores data entirely on the memory, therefore it can slow down or even crush the program under extreme cases. 

As a data analyst/statistician, large data sets are inevitable therefore researchers have worked on the issue of large data sets and come up with the following solutions; 

- Optimize the memory usage through data type conversion and row-wise processing 
- Processing large data sets in batches or in chunks. 
- Using memory efficient objects and programming tricks like nested functions, lazy loading(load data into memory when its needed) and use of garbage collection where objects that are no longer useful are disposed. 
- Partitioning and streaming by loading only small pieces of data into memory at any point in time. 
- Use specialized packages for large scale analysis like `data.table`. 
- Apply data sampling. Instead of processing the whole data at once. Take a random manageable sample from the data set and process it.

### Efficient Data Handling Techniques
Lets dive deep on how to work on large data sets in R by applying the popular methods in data science. 

#### Using data.table Package 
The `data.table` package delivers an improved version of a `data.frame` structure. The data structure from this package(has the same name, `data.table`) is high performance, memory efficient thus being more fit for large data sets than the `data.frame` kind of a data structure. 

Lets create a simple `data.table` from a random data set.
```{r echo=TRUE, message=FALSE, warning=FALSE}
# Load the library
library(data.table)

# Create the data.table 
student_dtb <- data.table(stuid = c(2, 5, 3, 4, 6, 7, 4, 2, 0),# student id
                          age = c(23, 45, 67, 23, 41, 43, 54, 67, 89)
                          )

student_dtb
```

A `data.frame` is created the same as a `data.table`
```{r}
# Create the data.frame
student_df <- data.table(stuid = c(2, 5, 3, 4, 6, 7, 4, 2, 0),# student id
                          age = c(23, 45, 67, 23, 41, 43, 54, 67, 89)
                          )

student_df
```

They almost look similar but they are more different in operations. Here are some of the differences;



| `data.table`                | `data.frame`                  |
|---------------------------- | ----------------------------- |
|faster than data.frame       | 20X slower than data.table    |
|Used for more complex data structures and big data | Used for smaller tables and matrices|
|Has built-in features like rolling joins and overlapping range| Lacks more features but friendly to a beginner|
|code efficient     | Utilizes more code to get something done| 
|`setDF(dt)` function is used to convert it to a data.frame where argument `dt` is the data.table| `setDT(df)` function is used to convert it to data.table where argument `df` is the data.frame|
| Syntax: `data.table()`     | Syntax: `data.frame()`         |

<-**Demonstrate basic data operations(check the guide)**->

<-**Add a practical exercise**->


#### Memory Management in R

<-**Discuss it**->
A good kind can be found [here](http://adv-r.had.co.nz/memory.html)

<-**Remember a practical exercise**->

### Reading and Writing Large Files 
#### Optimized File I/O





### Hands-On Exercises



## Data Cleaning and Transformation
### Introduction to Data Cleaning 
#### Common Data Issues 
Raw data from the field can be inaccurate and inconsistent and pose threats to data operations leading to false analysis and decision making. This type of data is know as dirty, unclean or rogue data. Here are some of the common data issues that render data unclean; 

- **Incomplete data**: Some of the data points may be missing or left blank. 
- **Duplicate data**: Some records in the data sets may occur multiple times in different sources.
- **Outdated data**: Some data values that may have relevant information times ago may now be obsolete and irrelevant. 
- **Inconsistent data formats**: Data may be in different ways across multiple data sources like records be presented in JSON formats, SQL tables, No SQL table or even in image formats(.png, .jpeg).
- **Outlier Values**: Outliers are extremely high or low values in the data set like a data set containing age of students having one student that is 304 years old or body temperature data set containing values of $-51^0C$. These values are always impossible and can indicate errors, anomalies and exceptions in the data sets. 

Data integrity is crucial in data analysis and can hinder the organizations' operations. The foundation of strategic planning, efficient operations and sound decision-making is based upon accurate, complete and consistent data. Here are some of the threats that unclean data pose to the organizations;


#### Data Cleaning techniques 
##### Handling Missing Data 
##### Dealing with Outliers
##### Data Transformation Techniques 
- **Normalization and Standardization**

- **Encoding Categorical Variables**

### Hands-On Exercises
